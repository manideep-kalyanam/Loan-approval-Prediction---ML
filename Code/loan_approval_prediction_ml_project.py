# -*- coding: utf-8 -*-
"""Loan Approval Prediction - ML Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tDe5GJyaJOQTXr7cIOiKXRzW7qdV1dm0
"""

# Commented out IPython magic to ensure Python compatibility.
#import the required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline

#import the train.csv file 
Train_set=pd.read_csv('datasets_659811_1164650_train_ctrUa4K.csv')
Train_set.head(5)

#Let's check the info of the Train_set
Train_set.info()

#Let's find out the count of null values w.r.t columns in Train_set
Train_set.isnull().sum()

#Shape of Train_set 
Train_set.shape

#import the test.csv file 
Test_set=pd.read_csv('datasets_659811_1164650_test_lAUu6dG.csv')
Test_set.head(5)

#Let's check the info of the Test_set
Test_set.info()

#Let's find out the count of null values w.r.t columns in Test_set 
Test_set.isnull().sum()

#Shape of Test_set 
Test_set.shape

"""#filling NaN(null) values

**Train_set**
"""

#filling the null values 
Train_set['Gender'] = Train_set['Gender'].fillna(Train_set['Gender'].mode()[0])
Train_set['Married'] = Train_set['Married'].fillna(Train_set['Married'].mode()[0])
Train_set['Dependents'] = Train_set['Dependents'].fillna(Train_set['Dependents'].mode()[0])
Train_set['Self_Employed'] = Train_set['Self_Employed'].fillna(Train_set['Self_Employed'].mode()[0])
Train_set['Credit_History'] = Train_set['Credit_History'].fillna(Train_set['Credit_History'].mode()[0])

Train_set['Loan_Amount_Term'] = Train_set['Loan_Amount_Term'].fillna(Train_set['Loan_Amount_Term'].mean())
Train_set['LoanAmount'] = Train_set['LoanAmount'].fillna(Train_set['LoanAmount'].mean())

Train_set.isnull().sum()

"""Now, there are no null values in Train_set"""

Train_set.head(5)

"""**Test_set**"""

#filling the null values 
Test_set['Gender'] = Test_set['Gender'].fillna(Test_set['Gender'].mode()[0])
Test_set['Dependents'] = Test_set['Dependents'].fillna(Test_set['Dependents'].mode()[0])
Test_set['Self_Employed'] = Test_set['Self_Employed'].fillna(Test_set['Self_Employed'].mode()[0])
Test_set['Credit_History'] = Test_set['Credit_History'].fillna(Test_set['Credit_History'].mode()[0])

Test_set['Loan_Amount_Term'] = Test_set['Loan_Amount_Term'].fillna(Test_set['Loan_Amount_Term'].mean())
Test_set['LoanAmount'] = Test_set['LoanAmount'].fillna(Test_set['LoanAmount'].mean())

Test_set.isnull().sum()

"""Now, there are no null values in Train_set"""

Test_set.head(5)

"""#Converting Dependents(changing '3+' to '3', then) to integer type"""

print('Train_set')
print(Train_set['Dependents'].value_counts())
print('Test_set')
print(Test_set['Dependents'].value_counts())

"""Let's just replace '3+' to '3' and change the data type of Dependents to integer."""

#Let's just replace 3+  to 3
Train_set['Dependents']=Train_set['Dependents'].replace('3+','3')
Test_set['Dependents']=Test_set['Dependents'].replace('3+','3')

#dtype of 'Dependents' is object let's change it to int type
Train_set['Dependents']=Train_set['Dependents'].astype(int)
Test_set['Dependents']=Test_set['Dependents'].astype(int)

print('Train_set',Train_set['Dependents'].unique())
print('Test_set',Test_set['Dependents'].unique())

Train_set.info()

"""We can observe that the datatype of dependents is changed from object to integer.

**Now let's check dependancy of Loan_Status on different Categorical columns in Train_set**

#Gender vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Gender

pd.crosstab(index=Train_set['Loan_Status'],columns=data['Gender'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Gender']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are Male**."""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Gender'],normalize='columns')

"""From the above crosstab, we will conclude that 
**70% of Males and 67% of Females are approved to loan** (loan is not approved to 30% of Males and 33% of Females)

#Marital Status vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Marital Status

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Married'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Married']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are Married.**"""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Married'],normalize='columns')

"""From the above crosstab, we will conclude that
 **70% of Married and 63% of Not-Married people are approved to loan** (loan is not approved to 30% of Married and 37% of Not-Married)

#Dependents vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Dependents

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Dependents'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Dependents']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are with 0 Dependents**."""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Dependents'],normalize='columns')

"""From the above crosstab, we will conclude that 68% of people with 0 dependents, 64% of people with 1 dependent, 75% of people with 2 dependent, 64% of people with 3 dependents are approved to loan.

#Education vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Education

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Education'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Education']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are Graduates.**"""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Education'],normalize='columns')

"""From the above crosstab, we will conclude that **70% of Graduates and 61% of Non-Graduates are approved to loan** (loan is not approved to 30% of Graduates and 39% of Non-Graduates)

#Employment vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Employment

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Self_Employed'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Self_Employed']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are Not Self-Employed.**"""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Self_Employed'],normalize='columns')

"""From the above crosstab, we will conclude that **68% of both Self Employed and Non-Self Employed are approved to loan** (loan is not approved to 32% of both Self Employed and Non-Self Employed are approved to loan)

#Credit_History vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Credit_History

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Credit_History'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Credit_History']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are with Credit_History = 1 (i.e., they might have repaid the previous loan)** and there are a very few people with Credit_History = 0."""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Credit_History'],normalize='columns')

"""From the above crosstab, we will conclude that **79% of people with CH=1 and only 8% of people with CH=0 are approved to loan** (loan is not approved to 21% of people with CH=1 and  **92% of people with CH=0**)

#Property_Area vs Loan_Status
"""

#Let's find the count of Loan_Status categories based on Property Area

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Property_Area'])

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Property_Area']).plot(kind='bar')
plt.ylabel('Count')

"""Among the loan approved people (i.e., Loan_Status=Y), **most of them are from Semiurban area.**"""

pd.crosstab(index=Train_set['Loan_Status'],columns=Train_set['Property_Area'],normalize='columns')

"""From the above crosstab, we will conclude that **61% of people from Rural area, 77% of people from Semiurban area and  66% of people from Urban area are approved to loan.**

#get_dummies (One hot encoding)

**Train_set**
"""

#Loan_Status
Train_set['Loan_Status']=pd.get_dummies(Train_set['Loan_Status'],drop_first=True)
#Yes=1, No=0

#Gender
Train_set['Gender']=pd.get_dummies(Train_set['Gender'],drop_first=True)
#Male=1, Female=0

#Marital Status
Train_set['Married']=pd.get_dummies(Train_set['Married'],drop_first=True)
#Married=1, Not Married=0

#Employment
Train_set['Self_Employed']=pd.get_dummies(Train_set['Self_Employed'],drop_first=True)
#Yes=1, No=0

#Education
Train_set['Education']=pd.get_dummies(Train_set['Education']=='Graduate',drop_first=True)
#Graduate=1, Not Graduate=0

#Property_Area
Train_set['Property_Area'] = Train_set['Property_Area'].map({'Rural':0,'Semiurban':1,'Urban':2})
#Rural=0, Semiurban=1, Urban=2

Train_set.head(5)

"""**Test_set**"""

#Gender
Test_set['Gender']=pd.get_dummies(Test_set['Gender'],drop_first=True)
#Male=1, Female=0

#Marital Status
Test_set['Married']=pd.get_dummies(Test_set['Married'],drop_first=True)
#Married=1, Not Married=0

#Employment
Test_set['Self_Employed']=pd.get_dummies(Test_set['Self_Employed'],drop_first=True)
#Yes=1, No=0

#Education
Test_set['Education']=pd.get_dummies(Test_set['Education']=='Graduate',drop_first=True)
#Graduate=1, Not Graduate=0

#Property_Area
Test_set['Property_Area'] = Test_set['Property_Area'].map({'Rural':0,'Semiurban':1,'Urban':2})
#Rural=0, Semiurban=1, Urban=2

Test_set.head(5)

"""#Data Correlation"""

#Let's check the correlation of Train_set
Train_set.corr()

#Now let's draw a heatmap to visualise the correlation
import seaborn as sns
plt.figure(figsize=(13,10))
plt.title("Data Correlation")
sns.heatmap(Train_set.corr())

"""We came to know that **LoanAmount - ApplicantIncome** and **Credit_History - Loan_Status** are related.

**Married-Gender** and **Married-Dependents** are also related to an extent.

#Features(X) and Label(Y)
"""

#Let's assign Loan_Status to Y

Y = Train_set['Loan_Status']
Y.head(5)

#Let's assign the columns other than Loan_Status and Loan_ID to Features

Features=Train_set.drop(['Loan_Status','Loan_ID'],axis=1)
Features.head(5)

"""**For Test_set:**"""

Test_Features = Test_set.drop(['Loan_ID'],axis=1)
Test_Features.head(5)

"""#Normalisation

Let's use StandardScaler technique to normalise the columns
"""

from sklearn.preprocessing import StandardScaler
SSC = StandardScaler()

# Let's Normalise all the Features

Norm_Features = SSC.fit_transform(Features)
print(Norm_Features)

"""**Normalising Test_set for later use:**"""

Norm_Test_Features = SSC.fit_transform(Test_Features)
print(Norm_Test_Features)

"""#Dimensionality Reduction (PCA)

Let's use PCA(Dimensionality Reduction technique) to reduce the number of columns.
"""

from sklearn.decomposition import PCA

pca = PCA(n_components = 5)
PCA_Norm_Features = pca.fit_transform(Norm_Features)
print(PCA_Norm_Features)

"""**Dimensionality Reduction fot Test_set for later use:**"""

PCA_Norm_Test_Features = pca.fit_transform(Norm_Test_Features)
print(PCA_Norm_Test_Features)

"""#Train_Test_Split"""

from sklearn.model_selection import train_test_split

#Let's assign that PCA_Norm_Features to X so that we could use this X to train our model

X = PCA_Norm_Features

#We have two different datasets for train(Train_set) and test(Train_set) data from the source
#But to measure the training accuracy and test accuracy while training the model, we split the given Train_set

X_train , X_test , y_train , y_test = train_test_split( X , Y , test_size=0.33)

"""#KNN"""

#import KNN

from sklearn.neighbors import KNeighborsClassifier

#Now let's plot a graph of Training and test accuracies varying the number of neighbours

Train_Accuracy = []
Test_Accuracy = []
for i in range(2,15):
  KNN = KNeighborsClassifier(n_neighbors = i)
  KNN.fit(X_train, y_train)

  Train_Acc = KNN.score(X_train, y_train)
  Test_Acc = KNN.score(X_test, y_test)

  Train_Accuracy.append(Train_Acc)
  Test_Accuracy.append(Test_Acc)

plt.plot(np.arange(2,15) , Train_Accuracy , label= 'Train_Acc')
plt.plot(np.arange(2,15) , Test_Accuracy , label = 'Test_Acc')
plt.xlabel('Number of neighbours')
plt.ylabel('Accuracy')
plt.legend()

# From the above graph we came to know that the best train and test accuracies are at n=7
KNN = KNeighborsClassifier(n_neighbors=7)

# fit the model
KNN.fit(X_train, y_train)

# Calculate and print the scores
print("Training Accuracy:", KNN.score(X_train, y_train))
print("Test Accuracy:", KNN.score(X_test, y_test))

"""So using KNN, we got accuracies as Training Accuracy: 0.8223, Test Accuracy: 0.8177

**Note:** We may not get this same all the times when we execute this because it depends on the splitting of data. Every time when we split the data, the results may differ.
"""

#Let's store the accuracies in some variables

knn_tr_acc=KNN.score(X_train,y_train)
knn_te_acc=KNN.score(X_test,y_test)
print(knn_tr_acc,knn_te_acc)

"""After trying different test cases, I got the best outcome from KNN at random_state=21 and n=3 : **Training Accuracy: 0.8394 , Test Accuracy: 0.8275**

#Logistic Regression
"""

#import Logistic regression 

from sklearn.linear_model import LogisticRegression
LRmodel = LogisticRegression()

# fit the model
LRmodel.fit(X_train, y_train)

# Calculate and print the scores
print("Training Accuracy:",LRmodel.score(X_train, y_train))
print("Testing Accuracy:",LRmodel.score(X_test, y_test))

"""From Logistic Regression we got Training Acc: 0.8102, Testing Acc: 0.8029

**Note**: We may not get this same all the times when we execute this because it depends on the splitting of data. Every time when we split the data, the results may differ.
"""

#Let's store the accuracies in some variables

lr_tr_acc=LRmodel.score(X_train,y_train)
lr_te_acc=LRmodel.score(X_test,y_test)
print(lr_tr_acc,lr_te_acc)

"""#SVM"""

#import SVC

from sklearn.svm import SVC

SVM = SVC()

# fit the model
SVM.fit(X_train , y_train)

# Calculate and print the scores
print("Training Accuracy:",SVM.score(X_train , y_train))
print("Testing Accuracy:",SVM.score(X_test , y_test))

"""From SVC, we got Training acc: 0.8223, Testing acc: 0.8029

**Note**: We may not get this same all the times when we execute this because it depends on the splitting of data. Every time when we split the data, the results may differ.
"""

#Let's store the accuracies in some variables

svm_tr_acc=SVM.score(X_train,y_train)
svm_te_acc=SVM.score(X_test,y_test)
print(svm_tr_acc,svm_te_acc)

"""#RandomForestClassifier"""

#import RandomForestClassifier

from sklearn.ensemble import RandomForestClassifier

RFC = RandomForestClassifier(max_depth=2, n_estimators=10, random_state=0)

# fit the model
RFC.fit(X_train, y_train)

# Calculate and print the scores
print('Training Accuracy:',RFC.score(X_train, y_train))
print('Test Accuracy:',RFC.score(X_test, y_test))

"""From RandomForestClassifier, we got Training Accuracy: 0.8150, Test Accuracy: 0.8029

**Note:** We may not get this same all the times when we execute this because it depends on the splitting of data. Every time when we split the data, the results may differ.
"""

#Let's store the accuracies in some variables

rfc_tr_acc=RFC.score(X_train,y_train)
rfc_te_acc=RFC.score(X_test,y_test)
print(rfc_tr_acc,rfc_te_acc)

"""#Gaussian Naive Baye's"""

#import GaussianNB

from sklearn.naive_bayes import GaussianNB

GNB = GaussianNB()

# fit the model
GNB.fit(X_train, y_train)

# Calculate and print the scores
print('Training Accuracy:',GNB.score(X_train,y_train))
print('Test Accuracy:',GNB.score(X_test,y_test))

"""From Gaussian Naive Bayes, we got Training Accuracy: 0.7931, Test Accuracy: 0.7881

**Note:** We may not get this same all the times when we execute this because it depends on the splitting of data. Every time when we split the data, the results may differ.
"""

#Let's store the accuracies in some variables

gnb_tr_acc=GNB.score(X_train,y_train)
gnb_te_acc=GNB.score(X_test,y_test)
print(gnb_tr_acc,gnb_te_acc)

"""#Comparision of Algorithms"""

#For comparision of algorithms, let's plot a bar chart

#Storing the names of the algorithms
Models = ['KNN' , 'LR' , 'SVM' , 'RFC' , 'GNB']

#Storing Training Accuracies
Final_Tr_Acc = [knn_tr_acc , lr_tr_acc , svm_tr_acc , rfc_tr_acc , gnb_tr_acc]

#Storing Test Accuracies
Final_Te_Acc = [knn_te_acc , lr_te_acc , svm_te_acc , rfc_te_acc , gnb_te_acc]

n = 5
type1 = (knn_tr_acc , lr_tr_acc , svm_tr_acc , rfc_tr_acc , gnb_tr_acc)
type2 = (knn_te_acc , lr_te_acc , svm_te_acc , rfc_te_acc , gnb_te_acc)


fig, ax = plt.subplots()
index = np.arange(n)
type1=np.array(Final_Tr_Acc)
type2=np.array(Final_Te_Acc)
bar_width = 0.35
opacity = 0.8

rects1 = plt.bar(index, type1, bar_width, alpha=opacity, align='edge', label='Train', color='black')

rects2 = plt.bar(index + bar_width , type2, bar_width, alpha=opacity , align='edge', label='Test', color='red')

plt.xlabel('Models')
plt.ylabel('Accuracy') 
plt.title('Accuracy of models')
plt.xticks(index + bar_width, ('KNN', 'LR', 'SVM', 'RFC' , 'GNB'))
plt.legend()

plt.plot(Models,Final_Tr_Acc,label='Training Accuracy')
plt.plot(Models,Final_Te_Acc,label='Test Accuracy')
plt.xlabel('Models')
plt.ylabel('Accuracy')
plt.legend()

"""#Conclusion

**Final Accuracies:**

**KNN:**  Training Accuracy: 0.8223, Test Accuracy: 0.8177

**Logistic Regression:**  Training Acc: 0.8102, Testing Acc: 0.8029

**SVM:** Training acc: 0.8223, Testing acc: 0.8029

**RandomForest Classifier:** Training Accuracy: 0.8150, Test Accuracy: 0.8029

**Gaussian Naive Bayes:** Training Accuracy: 0.7931, Test Accuracy: 0.7881

So, we conclude that **K-Nearest Neighbours model gives us the best result**. Test Accuracies of LR, SVM and RFC are equal.

#Predicting the Test_set
"""

#Recall the Dimensionality Reduction part 
#There we normalised the Test_set columns
#And we applied PCA on that Norm_Test_Features
#So, now assign that 'PCA_Norm_Test_Features' to predict the output of Test_data

input = PCA_Norm_Test_Features
print(input)

Output = KNN.predict(input)
print(Output)

Output.size

"""Hence, we can verify that it has predicted all the values and given the required output."""